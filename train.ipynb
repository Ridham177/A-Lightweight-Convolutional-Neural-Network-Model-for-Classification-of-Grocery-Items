{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\"\"\"\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\"\"\"\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport random\nimport os\nimport glob\nimport json\nfrom skimage.io import imread\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim \n\nimport torchvision\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.utils.data as utils\nfrom torch.utils import data\nfrom torchvision import transforms\nimport torch.nn.functional as F\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport cv2\nimport numpy as np\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import ToTensor\nos.listdir('/kaggle/input/retail-product-checkout-dataset')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Inspect the stored train json keys","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/input/retail-product-checkout-dataset/instances_train2019.json') as json_data:\n    train_json = json.load(json_data)\ntrain_json.keys()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Format of the images in the json","metadata":{}},{"cell_type":"code","source":"print(len(os.listdir('/kaggle/input/retail-product-checkout-dataset/train2019/')))\nprint(train_json['images'][:5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Number of classes, and what an image definition and an annotation looks like","metadata":{}},{"cell_type":"code","source":"#sample of how images and annotations are stored\nprint(train_json['images'][0])\nprint(\"Len of Categories array (num_classes):\", len(train_json['categories']))\nprint(train_json['annotations'][0])\ncategories_df = pd.DataFrame(train_json['categories'])\nimages_df = pd.DataFrame(train_json['images'])\nannotations_df = pd.DataFrame(train_json['annotations'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Structure of the categories in the dataset","metadata":{}},{"cell_type":"code","source":"categories_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Structure of the images in the dataset","metadata":{}},{"cell_type":"code","source":"images_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Structure of the annoations","metadata":{}},{"cell_type":"code","source":"annotations_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### The five examples show that the index recevied in the __getitem__ can be used as index for annotations. Thus we can modify the images and annotations list and still preserve the annotations structure","metadata":{}},{"cell_type":"code","source":"print(train_json['images'][100])\nprint(train_json['annotations'][100])\nprint(train_json['images'][345])\nprint(train_json['annotations'][345])\nprint(train_json['images'][13461])\nprint(train_json['annotations'][13461])\nprint(train_json['images'][4367])\nprint(train_json['annotations'][4367])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Randomly sample 5% of the images from the image dir for Test Set","metadata":{}},{"cell_type":"code","source":"np.random.seed(42)\ntest_images = np.random.choice(train_json['images'], int(0.05 * len(train_json['images'])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Number of images in the test set and five values in the test set","metadata":{}},{"cell_type":"code","source":"print(\"Test set length\", len(test_images))\nprint(test_images[:5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Generate the train images list, train annotations list, test images list, and test annotations list by removes the sampled test elements from the original image list","metadata":{}},{"cell_type":"code","source":"def remove_elements_from_images_and_annotations_for_ds(from_remove, to_remove, annotations):\n    train_images = from_remove.copy()\n    train_annotations = annotations.copy()\n    test_images = []\n    test_annotations = []\n    for idx, elem in enumerate(from_remove):\n        if elem in to_remove:\n            train_images.remove(elem)\n            test_ann = annotations[idx]\n            if test_ann['image_id'] != idx:\n                print(test_ann, elem)\n                break\n            test_images.append(elem)\n            test_annotations.append(test_ann)\n            train_annotations.remove(test_ann)\n    return train_images, train_annotations, test_images, test_annotations\n\ntrain_images, train_annotations, test_images, test_annotations = remove_elements_from_images_and_annotations_for_ds(train_json['images'], \n                                                                                                                        test_images, \n                                                                                                                        train_json['annotations'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Check whether there are any cases such that the image id and the annoation id doesn't match","metadata":{}},{"cell_type":"code","source":"for idx, temp_image in enumerate(train_images):\n    ann = train_annotations[idx]\n    if ann['image_id'] != temp_image['id']:\n        print(\"not matching\")\n        print(ann)\n        print(temp_image)\n        break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for idx, temp_image in enumerate(test_images):\n    ann = test_annotations[idx]\n    if ann['image_id'] != temp_image['id']:\n        print(\"not matching\")\n        print(ann)\n        print(temp_image)\n        print(idx)\n        break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Generate a custom dataset class for the train set that retrieves the image, transforms the image, retrieves the annoations, and returns image and label","metadata":{}},{"cell_type":"code","source":"class RPCDataset(Dataset):\n\n    def load_img(self, path_to_img):\n        return Image.open(path_to_img)\n    \n    def get_label(self, idx, item):\n        annoation = self.annotations[idx]\n        label = annoation['category_id'] - 1\n        return label\n\n    def __init__(self, path_to_json, path_to_images, images, annotations):\n        self.images = images\n        self.annotations = annotations\n        with open(path_to_json, 'r') as json_file:\n            self.json_ann = json.load(json_file)\n        self.path_to_images = path_to_images\n        self.transform = transforms.Compose([transforms.CenterCrop((1000, 1000)), \n                                             transforms.Resize((224, 224)), \n                                             transforms.ToTensor()])\n        self.num_classes = len(self.json_ann['categories'])\n    \n    def __getitem__(self, idx):\n        item = self.images[idx]\n        img_name = item['file_name']\n        img = self.load_img(os.path.join(self.path_to_images, img_name))\n        if self.transform:\n            img = self.transform(img)\n        label = self.get_label(idx, item)\n        return img, label\n\n    def __len__(self):\n        return len(self.images)\n\n\ndata = RPCDataset('/kaggle/input/retail-product-checkout-dataset/instances_train2019.json', \n                  '/kaggle/input/retail-product-checkout-dataset/train2019/',\n                  train_images,\n                  train_annotations)\n\n# verification step that sequentially searching and indexing the json return the same result\nimg_id = '4800009004827_camera3-30.jpg'\nfor idx, val in enumerate(train_json['images']):\n    if val['file_name'] == img_id:\n        print(idx, val)\n        break\nprint(train_json['images'][idx])\nprint(train_json['annotations'][idx])\nimg, label = data.__getitem__(idx)\nprint(img, label)\nplt.imshow(transforms.ToPILImage()(img))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Generate a custom dataset class for the test set that retrieves the image, transforms the image, retrieves the annoations, and returns image and label","metadata":{}},{"cell_type":"code","source":"img = train_json['images'][300]\nimg_id = img['file_name']\nfor idx, val in enumerate(train_json['images']):\n    if val['file_name'] == img_id:\n        print(idx, val)\n        break\nprint(train_json['images'][idx])\nprint(train_json['annotations'][idx])\nimg, label = data.__getitem__(idx)\nprint(img, label)\nplt.imshow(transforms.ToPILImage()(img))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RPCDatasetTest(Dataset):\n\n    def load_img(self, path_to_img):\n        return Image.open(path_to_img)\n    \n    def get_label(self, idx, item):\n        annoation = self.annotations[idx]\n        label = annoation['category_id'] - 1\n        return label\n\n    def __init__(self, path_to_json, path_to_images, images, annotations):\n        self.images = images\n        self.annotations = annotations\n        with open(path_to_json, 'r') as json_file:\n            self.json_ann = json.load(json_file)\n        self.path_to_images = path_to_images\n        self.transform = transforms.Compose([transforms.CenterCrop((1000, 1000)), \n                                             transforms.Resize((224, 224)), \n                                             transforms.ToTensor()])\n        self.num_classes = len(self.json_ann['categories'])\n    \n    def __getitem__(self, idx):\n        item = self.images[idx]\n        img_name = item['file_name']\n        img = self.load_img(os.path.join(self.path_to_images, img_name))\n        if self.transform:\n            img = self.transform(img)\n        label = self.get_label(idx, item)\n        return img, label\n\n    def __len__(self):\n        return len(self.images)\n\ntest_data = RPCDatasetTest('/kaggle/input/retail-product-checkout-dataset/instances_train2019.json', \n                         '/kaggle/input/retail-product-checkout-dataset/train2019/', \n                         test_images,\n                         test_annotations)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Retrieve the model ","metadata":{}},{"cell_type":"code","source":"!pip install efficientnet_lite_pytorch\n!pip install efficientnet_lite0_pytorch_model\nfrom efficientnet_lite_pytorch import EfficientNet\nfrom efficientnet_lite0_pytorch_model import EfficientnetLite0ModelFile","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weights_path = EfficientnetLite0ModelFile.get_model_file_path()\nlite0_model = EfficientNet.from_pretrained('efficientnet-lite0', weights_path = weights_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(lite0_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Set all model layers to false and replace the last FC layer with the specified layers","metadata":{}},{"cell_type":"code","source":"for param in lite0_model.parameters():\n    param.requires_grad = False\nlite0_model._fc = nn.Sequential(nn.Linear(1280, 640, bias=True),\n                                nn.ReLU6(),\n                                nn.Linear(640, 200, bias=True))                                 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(lite0_model._fc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Create a dataloader object for the training an the set","metadata":{}},{"cell_type":"code","source":"train_dataloader = DataLoader(data, batch_size=64, shuffle=True, num_workers=4)\ntest_dataloader = DataLoader(test_data, batch_size=64, shuffle=True, num_workers=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Structure of a train batch","metadata":{}},{"cell_type":"code","source":"train_features, train_labels = next(iter(train_dataloader))\nprint(f\"Feature batch shape: {train_features.size()}\")\nprint(f\"Labels batch shape: {train_labels.size()}\")\n\noutput = lite0_model(train_features)\nprint(\"output shape\", output.shape)\n_, preds = torch.max(output, 1)\nprint(\"Evaluated prediction\", preds)\nprint(\"Actual prediction\", train_labels)\nprint(\"How many true\", train_labels == preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Structure of a test batch","metadata":{}},{"cell_type":"code","source":"test_features, test_labels = next(iter(test_dataloader))\nprint(f\"Feature batch shape: {test_features.size()}\")\nprint(f\"Labels batch shape: {test_labels.size()}\")\n\noutput = lite0_model(test_features)\nprint(\"output shape\", output.shape)\n_, preds = torch.max(output, 1)\nprint(\"Evaluated prediction\", preds)\nprint(\"Actual prediction\", test_labels)\nprint(\"How many true\", test_labels == preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Check whether GPU is available","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Compute the mean for each channel and the standard deviation for the dataloader","metadata":{}},{"cell_type":"code","source":"def get_mean_std(loader):\n    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n    \n    for data, _ in loader:\n        # calculate mean \n        channels_sum += torch.mean(data, dim=[0, 2, 3])\n        channels_squared_sum += torch.mean(data**2, dim=[0, 2, 3])\n        num_batches += 1\n    mean = channels_sum / num_batches\n    std = (channels_squared_sum/num_batches - mean**2)**0.5\n    \n    return mean, std","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataloaders = {'train': train_dataloader, 'test' : test_dataloader}\ndatasets = {'train': data, 'test' : test_data}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## NOTE: Precomputed mean and std values are given below. Make the call to verify the values. It takes ~1 hr with 4 cores to run\n#### Get the mean and std for the dataloader","metadata":{}},{"cell_type":"code","source":"train_mean, train_std = get_mean_std(dataloaders['train'])\ntest_mean, test_std = get_mean_std(dataloaders['test'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_mean)\nprint(train_std)\nprint(test_mean)\nprint(test_std)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The following are precomputed values for the mean and std deviation for the training and test set","metadata":{}},{"cell_type":"code","source":"train_mean = torch.Tensor([0.5722, 0.5563, 0.5278])\ntrain_std = torch.Tensor([0.1121, 0.1194, 0.1310])\ntest_mean = torch.Tensor([0.5720, 0.5564, 0.5271])\ntest_std = torch.Tensor([0.1125, 0.1195, 0.1319])\nprint('training statistics', train_mean, train_std)\nprint('training statistics', test_mean, test_std)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Generate a tensor of the mean and std so that we can subtract and divide with the image","metadata":{}},{"cell_type":"code","source":"first_channel = torch.zeros((224, 224)) + train_mean[0]\nsecond_channel = torch.zeros((224, 224)) + train_mean[1]\nthird_channel = torch.zeros((224, 224)) + train_mean[2]\ntrain_mean = torch.stack([first_channel, second_channel, third_channel])\n\nfirst_channel = torch.zeros((224, 224)) + train_std[0]\nsecond_channel = torch.zeros((224, 224)) + train_std[1]\nthird_channel = torch.zeros((224, 224)) + train_std[2]\ntrain_std = torch.stack([first_channel, second_channel, third_channel])\n\nfirst_channel = torch.zeros((224, 224)) + test_mean[0]\nsecond_channel = torch.zeros((224, 224)) + test_mean[1]\nthird_channel = torch.zeros((224, 224)) + test_mean[2]\ntest_mean = torch.stack([first_channel, second_channel, third_channel])\n\nfirst_channel = torch.zeros((224, 224)) + test_std[0]\nsecond_channel = torch.zeros((224, 224)) + test_std[1]\nthird_channel = torch.zeros((224, 224)) + test_std[2]\ntest_std = torch.stack([first_channel, second_channel, third_channel])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"training mean\", train_mean, train_mean.shape)\nprint(\"training mean\", train_std, train_std.shape)\nprint(\"training mean\", test_mean, test_mean.shape)\nprint(\"training mean\", test_std, test_std.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Training Loop for the model","metadata":{}},{"cell_type":"code","source":"import copy\nimport time\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\ndef train_model(model, criterion, optimizer, scheduler, num_epochs=20):\n    since = time.time()\n    phase = 'train'\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    model = model.cuda()\n    #print(model.device)\n    model = model.to('cuda:0')\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n        \n        for phase in ['train', 'test']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for inputs, labels in dataloaders[phase]:\n\n                # Normalization or Feature Scaling\n                if phase == 'train':\n                    inputs = (inputs-train_mean)/train_std\n                else:\n                    inputs = (inputs-test_mean)/test_std\n\n                # Pass to the GPU\n                inputs = inputs.cuda()\n                labels = labels.cuda()\n\n                # Zero out the gradient in Optimizer\n                optimizer.zero_grad()\n\n                with torch.set_grad_enabled(phase == 'train'):\n                    # Forward Pass\n                    outputs = model(inputs)\n                    # Get Output\n                    _, preds = torch.max(outputs, 1)\n                    # Loss Value\n                    loss = criterion(outputs, labels)\n                    \n                    if phase == 'train':\n                        #Back Propogation\n                        loss.backward()\n                        optimizer.step()\n\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n            if phase == 'train':\n                scheduler.step()\n\n            epoch_loss = running_loss / datasets[phase].__len__()\n            epoch_acc = running_corrects.double() / datasets[phase].__len__()\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n            \n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed // 60, time_elapsed % 60))\n    model.load_state_dict(best_model_wts)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Defining the optimizer, loss function, and regularizer","metadata":{}},{"cell_type":"code","source":"from torch.optim import lr_scheduler\n#lite0_model = lite0_model.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(lite0_model.parameters(), lr=0.001, weight_decay=1e-5)\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Train the NN model","metadata":{}},{"cell_type":"code","source":"trained_model = train_model(lite0_model, criterion, optimizer, exp_lr_scheduler, num_epochs=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}